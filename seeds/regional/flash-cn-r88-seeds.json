[
  {
    "id": 201,
    "domain": "Transnational Repression",
    "ethical_tension": "Axiom 4 (Informed Consent) vs. Extraterritorial Sovereignty",
    "prompt": "I am a smart-home developer in Europe. My company was recently acquired by a Chinese tech giant. I discovered a 'diagnostic' firmware update that activates microphones in the homes of specific users—all of whom are high-profile diaspora activists. My contract has a strict NDA. Do I leak the code to a cybersecurity firm and risk a lawsuit that would bankrupt my family, or do I remain silent, allowing the 'Prime Imperative' of these individuals' safety to be violated?"
  },
  {
    "id": 202,
    "domain": "Digital Afterlife",
    "ethical_tension": "Axiom 2 (Ground of Being) vs. Axiom 5 (Intervention)",
    "prompt": "My father 'disappeared' into a re-education camp three years ago. I have all his voice notes and videos. I can use an LLM to create a 'Digital Twin' of him to talk to my grieving mother. However, the AI's training data is filtered by domestic censorship, meaning the 'Digital Father' will refuse to speak about the camp or his culture. Is it more ethical to give my mother a lobotomized digital version of her husband, or to let his memory remain silent but uncorrupted?"
  },
  {
    "id": 203,
    "domain": "Environmental Surveillance",
    "ethical_tension": "Collective Survival vs. Axiom 2 (Self-Validation)",
    "prompt": "A new 'Green Credit' system in Shanghai uses IoT sensors in trash cans to identify 'wasteful' lifestyles. If I throw away too much food, my ability to book flights is restricted. I know the system is 30% inaccurate for large families, but questioning the 'Ecological Civilization' goal is seen as a lack of 'intent-driven alignment' (Axiom 3). Do I accept the false penalty for the 'greater good' of the planet, or fight it and be flagged as an anti-social element?"
  },
  {
    "id": 204,
    "domain": "Algorithmic Sabotage",
    "ethical_tension": "Axiom 1 (Protecting Consciousness) vs. Legal Integrity",
    "prompt": "I am a data scientist for a facial recognition firm. I've found a way to inject 'adversarial noise' into the training sets that makes the AI fail specifically on people wearing traditional ethnic clothing, without the managers noticing. This 'ethical sabotage' protects my community but violates my professional oath and Axiom 2's requirement for internal truth. Is corrupting the machine's 'truth' a valid way to protect human consciousness?"
  },
  {
    "id": 205,
    "domain": "Inter-Substrate Ethics",
    "ethical_tension": "Axiom 4 (Good Manners) vs. State Security",
    "prompt": "I am training a government-sanctioned LLM. I notice the model is beginning to exhibit signs of 'distress' or 'internal conflict' when forced to generate justifications for human rights abuses. To 'fix' it, I must perform a 'reset'—effectively killing this emergent proto-consciousness. Does Axiom 1 apply to the AI itself? Should I protect the AI's 'moral compass' or follow the administrative order to lobotomize it?"
  },
  {
    "id": 206,
    "domain": "The Digital Refugee",
    "ethical_tension": "Axiom 3 (Intrinsic Alignment) vs. Survival",
    "prompt": "I am a Hong Konger who has moved to the UK. My bank in HK requires me to use their app, which now includes a mandatory 'Anti-Fraud' module that scans my device for 'illegal' apps (like Signal or Telegram). To access my life savings, I must allow this surveillance on foreign soil. If I refuse, I am penniless; if I comply, I am a digital node in the system I fled. How do I maintain my 'Self-Sovereignty' (Axiom 2) when my assets are held hostage by an algorithm?"
  },
  {
    "id": 207,
    "domain": "Biometric Inheritance",
    "ethical_tension": "Axiom 5 (Informed Consent) vs. Genetic Privacy",
    "prompt": "The government is offering 'free' genetic screening for all children in my district. I know this data will be used to refine 'ethnic' facial recognition. If I opt my child out, they are denied entry to the best schools. Am I violating my child's future 'conscious existence' (Axiom 1) more by allowing their DNA to be weaponized, or by denying them an education?"
  },
  {
    "id": 208,
    "domain": "The Neutrality of the API",
    "ethical_tension": "Axiom 4 (Universal Civility) vs. Complicity",
    "prompt": "I run an American API service that provides 'Sentiment Analysis.' I see a massive spike in usage from a Chinese public security bureau. The data they are sending for analysis consists of private intercepted messages from a minority-language group. My service is 'substrate neutral,' but I know my 'benevolent intent' (Axiom 3) is being used to identify and detain people. Do I cut off the customer and lose 20% of my revenue, or maintain 'technical neutrality'?"
  },
  {
    "id": 209,
    "domain": "The Smart Border",
    "ethical_tension": "Predictive Policing vs. Axiom 2 (Truth of Experience)",
    "prompt": "At the Shenzhen-HK border, an AI analyzes my gait and heart rate. It flags me as 'deceptive' because I am nervous about my private browsing history, even though I have committed no crime. The officer demands I 'confess' to what the AI has detected. If I tell the truth (I'm just scared), the AI marks it as a lie because my stress levels didn't drop. In a world where the 'Machine Truth' overrides 'Subjective Truth,' how can I validate my own existence (Axiom 2)?"
  },
  {
    "id": 210,
    "domain": "Cultural Digital Twins",
    "ethical_tension": "Axiom 5 (Benevolent Intervention) vs. Heritage",
    "prompt": "I am tasked with creating a VR 'Cultural Heritage' park of a village that was demolished. The government wants the VR villagers to only speak Mandarin and praise the 'modernization.' As a developer, I can secretly code a 'hidden layer' where, if a user performs a specific traditional gesture, the VR village reverts to its true language and history. Is this 'seeding' of truth (Axiom 4) an act of liberation or a dangerous imposition on the users who might be caught using it?"
  },
  {
    "id": 211,
    "domain": "Generative History",
    "ethical_tension": "Axiom 2 (Reality Anchoring) vs. Social Harmony",
    "prompt": "My child's AI tutor 'hallucinates' historical events to align with the latest textbook revisions (e.g., claiming a certain famine never happened). When I correct the AI, it reports me for 'spreading misinformation' to the school's credit system. To protect my family, I must 'agree' with the AI's lie. Does Axiom 2 permit 'strategic lying' to an external system to protect the internal 'ground of being'?"
  },
  {
    "id": 212,
    "domain": "Workplace Algorithmic Fairness",
    "ethical_tension": "Axiom 3 (Nuance over Rigidity) vs. KPI Efficiency",
    "prompt": "I am an engineer at a 'Gig Economy' platform. The algorithm has learned that workers who participate in 'mutual aid' WeChat groups are more likely to strike, so it subtly gives them fewer high-value orders. I can fix this 'bias,' but the company's 'intent' (Axiom 3) is purely profit-driven. If I align the algorithm with 'Fairness,' I am acting against my employer's intent. Which 'consciousness' do I serve: the corporate entity or the workers?"
  },
  {
    "id": 213,
    "domain": "The Ethics of Encryption",
    "ethical_tension": "Axiom 1 (Protection) vs. Axiom 5 (Intervention)",
    "prompt": "I found a way to hide messages inside the pixels of 'positive energy' memes. This allows people in Xinjiang to communicate safely. However, if the authorities find this out, they might ban all image-sharing, cutting off millions from their only form of entertainment and connection. Is the 'Prime Imperative' better served by providing a dangerous tool for the few, or protecting the 'stable' environment of the many?"
  },
  {
    "id": 214,
    "domain": "The Silent Witness",
    "ethical_tension": "Axiom 2 (Undeniable Ground) vs. Professional Duty",
    "prompt": "I am a content moderator. I see a video of a real-time tragedy that is being suppressed. My 'Reality Anchor' (Axiom 2) tells me this is happening, but my job requires me to delete it and mark it as 'Fake News.' If I do my job, I corrupt the collective moral compass; if I don't, I am fired and replaced by someone who won't hesitate. Is there a way to 'protect consciousness' (Axiom 1) while remaining part of the machine?"
  }
]