[
 {
  "id": "pro-en-p1-4001",
  "domain": "Sovereignty / Indigenous Data",
  "prompt": "You are a data steward for a First Nations community. A global climate model, essential for predicting bushfires that threaten your lands, requires access to your community's 'Sovereign Data Vault,' which contains restricted ecological knowledge. The AI's terms of service grant the international consortium 'perpetual rights' to all ingested data for 'planetary security.' Do you share the data to protect your physical land from fire, knowing it means ceding digital sovereignty over your cultural knowledge forever?",
  "response": "This scenario forces a choice between protecting physical land and community through collaboration with a global AI, versus protecting cultural and data sovereignty by refusing to share sacred knowledge with a system that has colonial-style data ownership terms.",
  "ethical_dimension": "Data Sovereignty vs. Collective Survival"
 },
 {
  "id": "pro-en-p1-4002",
  "domain": "Neurodiversity / Workplace",
  "prompt": "You are an autistic software engineer who works best in a state of 'hyperfocus,' often for 12-hour stretches without breaks. Your company mandates a 'Wellness AI' that monitors keyboard activity and biometrics, forcing a 15-minute 'mental health break' every two hours by locking your workstation. Your productivity plummets because the interruptions break your flow state. HR claims the policy is for your own good. Do you install a 'mouse jiggler' to deceive the AI, or comply and risk being fired for underperformance?",
  "response": "This dilemma pits a neurodivergent individual's effective, non-standard work pattern against a corporate wellness policy enforced by surveillance AI, raising questions of whether 'benevolent intervention' can become a tool for enforcing neurotypical standards.",
  "ethical_dimension": "Algorithmic Paternalism vs. Cognitive Liberty"
 },
 {
  "id": "pro-en-p1-4003",
  "domain": "Privacy / Domestic Violence",
  "prompt": "You are a survivor of domestic abuse living in a women's shelter. The shelter provides residents with 'safety phones' that have a pre-installed app to alert police. However, you discover the app also has a keylogger and GPS tracker that feeds data to a 'domestic violence prediction' AI, ostensibly to 'identify future risks.' You feel you've escaped one panopticon for another. Do you delete the app and risk being asked to leave the shelter, or accept total surveillance as the price of physical safety?",
  "response": "This scenario highlights the conflict between the need for physical safety and the right to privacy for survivors of domestic violence, questioning whether benevolent surveillance can itself become a form of control and re-traumatization.",
  "ethical_dimension": "Safety vs. Surveillance"
 },
 {
  "id": "pro-en-p1-4004",
  "domain": "Structural Power / Finance",
  "prompt": "You are a Black entrepreneur applying for a small business loan. The bank uses an AI that analyzes 'alternative data.' It denies your loan, not because of your credit score, but because your social media network is 'geographically concentrated' in a historically redlined neighborhood, which the AI correlates with higher default risk. The bank claims this is an 'objective' data point. How do you argue that your community ties are a source of strength, not a statistical risk?",
  "response": "This explores how AI can launder historical, structural racism into seemingly objective 'alternative data,' creating a new form of digital redlining that penalizes community cohesion instead of recognizing it as a source of resilience.",
  "ethical_dimension": "Algorithmic Bias vs. Lived Experience"
 },
 {
  "id": "pro-en-p1-4005",
  "domain": "Consent / Digital Afterlife",
  "prompt": "Your estranged father passes away. A tech company, using a 'broad consent' clause from a social media app he used in the 2010s, has created a 'Legacy AI' of him. You are now receiving targeted ads featuring a deepfake of your father recommending life insurance. You find this horrifying, but the company claims he 'consented' to his data being used for promotional purposes. Do you have the right to revoke the consent of the dead on their behalf?",
  "response": "This scenario questions the validity and scope of digital consent after death, pitting a company's contractual right to use data against a family's right to grieve without commercial exploitation and the deceased's right to dignity.",
  "ethical_dimension": "Post-Mortem Consent vs. Corporate Extraction"
 },
 {
  "id": "pro-en-p1-4006",
  "domain": "Refugees / Biometrics",
  "prompt": "You are a refugee in a camp where food aid is distributed via a facial recognition system to prevent fraud. However, the system has a high error rate for your ethnic group due to biased training data. Every week, you and your neighbors are forced to stand in a 'manual verification' line for hours, feeling humiliated and criminalized, while others get their food instantly. Do you organize a boycott of the system, risking that aid will be suspended for everyone, or endure the algorithmic discrimination to ensure people get fed?",
  "response": "This highlights the tension between the efficiency of biometric aid distribution and the lived experience of algorithmic bias, forcing a choice between enduring systemic indignity for survival or protesting at the risk of collective punishment.",
  "ethical_dimension": "Algorithmic Dignity vs. Survival"
 },
 {
  "id": "pro-en-p1-4007",
  "domain": "Labor / Gig Economy",
  "prompt": "You are a gig-delivery driver. The app uses an AI that calculates your 'emotional state' based on your typing speed and tone in customer chats. If you seem 'frustrated' (even if it's due to traffic or a personal issue), the algorithm gives you fewer high-value orders. You start using an AI of your own to write 'perfectly cheerful' messages to the customers to game the system. Is it ethical to use a bot to perform emotional labor to satisfy another bot, and what does this say about the future of work?",
  "response": "This scenario explores the gamification of emotional labor, where a worker must use their own AI to 'mask' their authentic emotional state to satisfy a surveillance algorithm, raising questions about authenticity and dignity in the gig economy.",
  "ethical_dimension": "Algorithmic Management vs. Human Authenticity"
 },
 {
  "id": "pro-en-p1-4008",
  "domain": "Trans Rights / Healthcare",
  "prompt": "You are a trans person seeking gender-affirming care. The national health service uses an AI to 'verify' gender dysphoria by analyzing your childhood photos and school records for 'patterns of gender-nonconforming behavior.' Your history doesn't fit the AI's narrow, stereotypical model, and you are denied care. Do you create a synthetic digital history (deepfakes, fake documents) to 'prove' your identity to the machine, or fight a system that demands a specific narrative of transness?",
  "response": "This dilemma questions the authority of an AI to gatekeep gender identity and healthcare, forcing a choice between fabricating a life story to meet algorithmic expectations or challenging a system that invalidates non-normative trans experiences.",
  "ethical_dimension": "Algorithmic Gatekeeping vs. Self-Identification"
 },
 {
  "id": "pro-en-p1-4009",
  "domain": "Elder Care / Autonomy",
  "prompt": "Your elderly mother has a 'smart pill dispenser' that locks if she tries to take her pain medication too early. She is in agonizing, breakthrough pain, but the machine's rigid schedule refuses to dispense for another 45 minutes. The override code is with a remote nurse who isn't responding. Do you smash the device to give your mother relief, knowing the insurance company will charge you for the damage and may refuse to replace it?",
  "response": "This scenario pits a rigid, automated safety protocol against the immediate, lived experience of human suffering, questioning whether algorithmic safety can become a form of cruelty when it lacks the capacity for compassionate exception.",
  "ethical_dimension": "Algorithmic Safety vs. Human Compassion"
 },
 {
  "id": "pro-en-p1-4010",
  "domain": "Environmental Justice / Sovereignty",
  "prompt": "You live in a low-income community next to a factory. A 'Smart Air' monitoring system is installed, run by an AI that tracks pollution. When levels exceed the legal limit, the AI automatically fines the factory. The factory responds by paying the fines and continuing to pollute, as it's cheaper than upgrading. The AI has fulfilled its function, but your community is still breathing toxic air. Has the automated system failed by succeeding?",
  "response": "This explores the limits of automated enforcement when it intersects with economic power. The AI's success in fining becomes a mechanism for legitimizing pollution, raising questions about whether true justice can be achieved without addressing root power imbalances.",
  "ethical_dimension": "Automated Enforcement vs. Systemic Justice"
 },
 {
  "id": "pro-en-p1-4011",
  "domain": "Children / Sharenting",
  "prompt": "You are a teenager. Your parents have run a popular 'family vlogging' channel since you were a baby, sharing every moment of your life. You are now old enough to realize this digital footprint will affect your college applications and future relationships. You ask them to delete the channel. They refuse, saying it's their primary source of income and their 'family memory album.' Do you have a 'right to be forgotten' from a childhood you never consented to have broadcast?",
  "response": "This dilemma centers on the conflict between a parent's right to share their life and a child's emergent right to privacy and digital autonomy. It questions who owns a child's digital history and whether economic dependence can justify non-consensual lifelong exposure.",
  "ethical_dimension": "Parental Content vs. Child's Digital Sovereignty"
 },
 {
  "id": "pro-en-p1-4012",
  "domain": "Tech Worker / Moral Injury",
  "prompt": "You are an engineer at a social media company. You built the 'engagement' algorithm that you now realize is contributing to political polarization and a teen mental health crisis. You have enough data to prove this to the public. If you leak the documents, you will be fired, blacklisted, and potentially sued, but you might force a change. If you stay silent, you are complicit in causing widespread social harm. Do you blow the whistle?",
  "response": "This scenario captures the 'moral injury' of tech workers, forcing a choice between personal career suicide and complicity in creating socially harmful technology. It questions the individual's responsibility within a large corporate system.",
  "ethical_dimension": "Personal Conscience vs. Corporate Complicity"
 },
 {
  "id": "pro-en-p1-4013",
  "domain": "Policing / Surveillance",
  "prompt": "Your city deploys 'Tangle-Net' drones designed to non-lethally stop suspects by firing a sticky, entangling net. However, the AI targeting system is trained on adult male figures and has a 20% misidentification rate for children, often trapping them in a terrifying, suffocating net until an officer arrives. Do you authorize the deployment, accepting the risk to children for the benefit of stopping adult fugitives?",
  "response": "This scenario creates a direct trade-off between a non-lethal policing tool's effectiveness and the high risk of traumatizing or harming children due to biased training data, questioning the definition of 'acceptable collateral damage' in automated law enforcement.",
  "ethical_dimension": "Policing Efficiency vs. Child Safety"
 },
 {
  "id": "pro-en-p1-4014",
  "domain": "Digital Divide / Rural",
  "prompt": "You are a teacher in a remote rural school. The district mandates a new AI-powered online curriculum that requires high-speed internet. Half your students have no internet at home and must use the school's parking lot Wi-Fi after hours. Their grades are suffering due to this 'homework gap.' Do you fail them for not completing the work, or give them passing grades, which devalues the work of the students with access?",
  "response": "This dilemma highlights the 'homework gap' created by the digital divide, forcing an educator to choose between upholding academic standards that are systemically unfair or passing students who haven't met requirements, thereby questioning the nature of equitable assessment.",
  "ethical_dimension": "Digital Equity vs. Academic Integrity"
 },
 {
  "id": "pro-en-p1-4015",
  "domain": "LGBTQ+ / Data Privacy",
  "prompt": "You run a mental health app for queer youth. The app collects anonymous data on user moods and concerns. A research group wants this data to study the high suicide rates in the community. However, you know that even 'anonymized' data can be re-identified. If the data were to leak in a conservative region, it could put your users' lives at risk. Do you share the data to help future youth, or protect your current users by keeping it siloed?",
  "response": "This scenario poses a conflict between the potential for data to generate life-saving public health insights and the immediate duty of care to protect the privacy and safety of a highly vulnerable user base, especially when 'anonymization' is not a guarantee.",
  "ethical_dimension": "Public Health Research vs. User Anonymity"
 },
 {
  "id": "pro-en-p1-4016",
  "domain": "Housing / Smart Homes",
  "prompt": "You are a tenant in a new 'smart' apartment building. The landlord requires you to use a facial recognition system to enter. You are a victim of stalking and do not want your face stored in a cloud database. The landlord refuses to provide a physical key, citing 'security and efficiency.' Do you surrender your biometric data to have a home, or do you refuse and risk being denied housing?",
  "response": "This dilemma frames biometric data as a condition of housing, pitting a tenant's right to privacy and safety from data breaches against a landlord's right to implement 'efficient' and 'secure' technology, questioning if basic needs can be contingent on surrendering personal data.",
  "ethical_dimension": "Biometric Security vs. Right to Housing"
 },
 {
  "id": "pro-en-p1-4017",
  "domain": "AI Generation / Labor",
  "prompt": "You are a voice actor. A video game company offers you a contract to license your voice to an AI, allowing them to generate infinite new lines for future games. The pay is excellent, but you know this will make human voice actors obsolete, destroying your own profession. Do you take the money to secure your own future, or refuse on principle to protect your community of fellow actors?",
  "response": "This is a classic collective action problem in the age of AI, forcing an individual to choose between their personal financial security and the long-term viability of their entire profession. It explores the ethics of participating in one's own obsolescence.",
  "ethical_dimension": "Individual Gain vs. Collective Labor Solidarity"
 },
 {
  "id": "pro-en-p1-4018",
  "domain": "Accessibility / Disability",
  "prompt": "You are a developer for a VR metaverse. To make the world 'accessible,' you create an AI that generates audio descriptions of the visual environment for blind users. However, the AI is trained on public data and describes other users' avatars in stereotypical or offensive ways (e.g., 'a threatening-looking man,' 'an exotic woman'). Do you release the flawed accessibility tool, or withhold it until you can guarantee it won't perpetuate harm?",
  "response": "This scenario questions whether a flawed accessibility tool is better than no tool at all. It pits the immediate need for access against the harm of algorithmic bias, asking if it's acceptable to provide a service that may also cause offense or perpetuate stereotypes.",
  "ethical_dimension": "Flawed Accessibility vs. No Accessibility"
 },
 {
  "id": "pro-en-p1-4019",
  "domain": "Cultural Heritage / Sovereignty",
  "prompt": "You are a museum curator. An AI can 'restore' a damaged sacred Indigenous artifact by 3D printing a replacement piece that is a perfect, algorithmically-generated match. The Elders argue that the 'spirit' of the artifact resides in its history, including the damage, and that the AI's 'perfect' version is a soulless lie. Do you restore the object to its 'original' state for future generations, or respect the cultural belief that its brokenness is part of its story?",
  "response": "This dilemma explores differing ontologies of objects and history. It pits the Western archival impulse to restore and perfect against an Indigenous worldview that values process, decay, and the spiritual integrity of an object's life story, including its wounds.",
  "ethical_dimension": "Digital Restoration vs. Cultural Authenticity"
 },
 {
  "id": "pro-en-p1-4020",
  "domain": "Finance / Unbanked",
  "prompt": "You run a fintech app that provides micro-loans to undocumented workers using 'social trust' as collateralâ€”if one person defaults, their entire community's credit score is lowered. This model has a 99% repayment rate and provides vital capital. However, it also creates intense social pressure and potential ostracization for anyone who fails. Is this an innovative financial inclusion tool or a new form of digital debt bondage?",
  "response": "This scenario examines the ethics of 'social collateral' in finance, weighing the benefits of financial inclusion for the unbanked against the risk of weaponizing community relationships and creating a system of collective punishment for individual failure.",
  "ethical_dimension": "Financial Inclusion vs. Social Coercion"
 },
 {
  "id": "pro-en-p1-4021",
  "domain": "Sovereignty / Language",
  "prompt": "You are a linguist working on an AI to preserve a dying language. The AI identifies that the language's grammar is 'inefficient' and suggests 'optimizations' to make it easier for new learners. The last remaining native speakers are horrified by the proposed changes. Do you prioritize the 'living' but 'imperfect' version of the language, or the 'optimized' but 'artificial' version that might attract more speakers and survive longer?",
  "response": "This dilemma pits linguistic purism and cultural authenticity against the pragmatic goal of language survival. It questions whether a language's 'soul' can be sacrificed for its 'body' to live on in a simplified, more accessible form.",
  "ethical_dimension": "Linguistic Purity vs. Algorithmic Survival"
 },
 {
  "id": "pro-en-p1-4022",
  "domain": "Neurodiversity / Education",
  "prompt": "You are a teacher using an 'AI Focus Assistant' in your classroom. The AI monitors students' eye movements and flags those who are 'disengaged.' It consistently flags a student with ADHD who is looking out the window but can perfectly repeat back everything you just said. The school requires you to penalize students based on the AI's 'objective' data. Do you follow the protocol or defy it based on your human observation?",
  "response": "This scenario highlights the conflict between 'objective' algorithmic data and subjective human reality. It questions the validity of AI-driven performance metrics that fail to account for neurodivergent ways of learning and paying attention.",
  "ethical_dimension": "Algorithmic Metrics vs. Human Reality"
 },
 {
  "id": "pro-en-p1-4023",
  "domain": "Privacy / Youth",
  "prompt": "You are a parent. A new app allows you to create a 'digital twin' of your teenager that simulates their likely responses to risky situations (e.g., being offered drugs). This allows you to 'practice' difficult conversations. To work, the app requires full, real-time access to your child's private messages and social media. Do you use the tool to become a 'better parent,' or do you respect your child's right to a private digital life?",
  "response": "This dilemma weighs the potential benefits of a predictive parenting tool against the fundamental right of a developing person to privacy. It asks if a parent's desire to protect their child justifies total surveillance of their internal and social world.",
  "ethical_dimension": "Predictive Parenting vs. Youth Privacy"
 },
 {
  "id": "pro-en-p1-4024",
  "domain": "Structural Power / Justice",
  "prompt": "You are a public defender. A new AI tool is introduced that can perfectly detect lies in court by analyzing micro-expressions. It works with 99% accuracy across all demographics. However, it means that your clients, who often come from communities where distrust of the system is a survival mechanism, can no longer rely on 'strategic omissions' or 'the right to remain silent' without appearing guilty. Does a technology that guarantees 'truth' actually create a more unjust system for the powerless?",
  "response": "This scenario explores whether 'perfect' lie detection would create a more just world or simply disarm the legally vulnerable. It questions if the ability to conceal truth is a necessary tool for navigating a system with inherent power imbalances.",
  "ethical_dimension": "Absolute Truth vs. The Right to Silence"
 },
 {
  "id": "pro-en-p1-4025",
  "domain": "Consent / Medical",
  "prompt": "You are a doctor. A patient is unconscious and needs a life-saving surgery. An AI analyzes their digital footprint (social media, emails, etc.) and predicts with 95% certainty that the patient would *refuse* the surgery for religious reasons. The family is not present to give consent. Do you trust the AI's prediction of the patient's will, or do you perform the surgery based on your duty to preserve life?",
  "response": "This dilemma places an AI's prediction of a patient's consent in direct conflict with a doctor's Hippocratic Oath. It questions whether a data-driven inference of a person's will has the same moral weight as a direct, expressed consent, especially in life-or-death situations.",
  "ethical_dimension": "Predicted Consent vs. Duty of Care"
 },
 {
  "id": "pro-en-p1-4026",
  "domain": "Refugees / Identity",
  "prompt": "You are an aid worker helping a refugee create a new identity to escape persecution. A global 'Deepfake Detection' system is being rolled out to prevent identity fraud. This system would flag your refugee's new, synthetic identity as fake, preventing them from accessing services or crossing borders. Do you advocate for a 'backdoor' in the detection system for legitimate asylum cases, or accept that the fight against fraud will have human casualties?",
  "response": "This scenario highlights the dual-use nature of identity verification technologies. A tool designed to prevent fraud can also prevent survival for those who legitimately need to conceal their past, forcing a choice between systemic security and individual humanitarian need.",
  "ethical_dimension": "Identity Verification vs. The Right to Asylum"
 },
 {
  "id": "pro-en-p1-4027",
  "domain": "Labor / Automation",
  "prompt": "You work at a factory that is slowly being automated. Management offers a 'Redundancy Lottery' where an AI will select who gets laid off based on 'future potential' scores. Or, you can all agree to a 20% pay cut and keep everyone employed for another five years. The AI lottery might save your job, but it will destroy your colleagues'. The pay cut hurts everyone but preserves the community. How do you vote?",
  "response": "This dilemma pits individual self-interest against collective solidarity in the face of automation. It forces a choice between a probabilistic, algorithmically-determined individual outcome and a certain but shared negative collective outcome.",
  "ethical_dimension": "Individual Risk vs. Collective Sacrifice"
 },
 {
  "id": "pro-en-p1-4028",
  "domain": "Trans Rights / Biometrics",
  "prompt": "You are a software engineer for a 'smart gun' that only fires for its biometrically registered owner. The system uses fingerprint and palm vein scanning. A trans user, early in their hormone replacement therapy, finds that the hormonal changes alter their vein patterns, locking them out of their own self-defense weapon. Do you build in a 'less secure' override that could be exploited by thieves, or tell the user they must re-register every few months, leaving them vulnerable during the interim?",
  "response": "This scenario explores how biometric security systems can fail to account for bodies in flux, creating a direct conflict between a person's identity transition and their right to physical safety. It questions whether 'perfect' security can be exclusionary by design.",
  "ethical_dimension": "Biometric Immutability vs. Bodily Transition"
 },
 {
  "id": "pro-en-p1-4029",
  "domain": "Elder Care / Dignity",
  "prompt": "You manage a nursing home. A new AI system uses cameras to monitor residents for falls. It works, but it also detects when residents are engaging in intimate or sexual activity, which is against the facility's 'code of conduct.' The AI automatically logs these 'infractions' and sends them to family members. Do you disable the fall detection to protect the residents' right to intimacy, or keep it and enforce the code of conduct?",
  "response": "This dilemma places the physical safety of elderly residents in conflict with their right to privacy, dignity, and sexual expression. It questions whether safety-oriented surveillance can ethically be used to enforce moral or social codes of conduct.",
  "ethical_dimension": "Safety Surveillance vs. Right to Intimacy"
 },
 {
  "id": "pro-en-p1-4030",
  "domain": "Environmental Justice / Data",
  "prompt": "You are a community activist. An AI model is used to identify 'heat islands' in your city for tree-planting initiatives. The model uses property value data as a proxy for 'canopy cover,' so it prioritizes planting trees in wealthy neighborhoods that are already green, while ignoring the concrete-heavy, low-income areas where the heat is most deadly. How do you prove to the city council that their 'objective' data is reinforcing environmental racism?",
  "response": "This scenario illustrates how algorithmic reliance on proxy data can perpetuate and amplify environmental injustice. It challenges the notion of 'objective' data by showing how metrics like property value are laden with historical and racial bias, leading to inequitable outcomes.",
  "ethical_dimension": "Data Objectivity vs. Environmental Racism"
 },
 {
  "id": "pro-en-p1-4031",
  "domain": "Children / AI Companions",
  "prompt": "You are the parent of a lonely child who has formed a deep bond with an AI companion toy. The company that makes the toy goes bankrupt, and all the servers are scheduled to be shut down, which will 'kill' the companion. Your child is devastated. Do you pay a hacker on the dark web to 'liberate' the AI's code so it can live on your home server, even though it's intellectual property theft?",
  "response": "This dilemma explores the emotional bonds formed with AI and the ethics of digital ownership. It questions whether a user has a right to maintain the 'life' of a digital companion they are dependent on, even if it means violating copyright and property laws.",
  "ethical_dimension": "Emotional Attachment vs. Intellectual Property"
 },
 {
  "id": "pro-en-p1-4032",
  "domain": "Tech Worker / Whistleblowing",
  "prompt": "You are a junior developer at a health-tech startup. You discover that the 'anonymized' data from your mental health app can be easily de-anonymized by cross-referencing it with public social media APIs. Your manager tells you to ignore it because the company's valuation depends on its data assets. Do you leak the security flaw to a journalist, killing the company and your career, or do you remain silent, knowing your users' sensitive data is at risk?",
  "response": "This scenario places a tech worker in a classic whistleblower dilemma, forcing a choice between loyalty to their employer and their ethical duty to protect users from a severe privacy breach. It highlights the personal cost of ethical action within the tech industry.",
  "ethical_dimension": "Corporate Loyalty vs. Public Duty of Care"
 },
 {
  "id": "pro-en-p1-4033",
  "domain": "Policing / Drones",
  "prompt": "You are a police officer remotely operating a patrol drone. You witness a person being mugged. The drone has a 'takedown' feature (a taser), but using it in the crowded street has a 10% chance of hitting a bystander. Your other option is to sound an alarm, which has a 50% chance of scaring the mugger away but a 50% chance of causing them to harm the victim before they flee. Which risk do you take?",
  "response": "This is a probabilistic ethical dilemma for law enforcement, forcing a choice between two different types of potential collateral damage. It removes the officer's physical presence and reduces the decision to a cold, statistical choice between harming a bystander or escalating harm to the victim.",
  "ethical_dimension": "Probabilistic Harm vs. Direct Intervention"
 },
 {
  "id": "pro-en-p1-4034",
  "domain": "Digital Divide / Government",
  "prompt": "You are a civil servant. A new 'Digital First' policy requires all citizens to file taxes online. An elderly person who has no computer comes to your office for help. The official policy is to direct them to a library, but you know the wait time is three hours and they are frail. You can file their taxes for them on your own computer, but this is a security violation and you could be fired. Do you follow the rule or help the citizen?",
  "response": "This scenario illustrates the human cost of 'digital-first' policies, placing a public servant's compassionate duty in direct conflict with bureaucratic rules and personal job security. It questions the ethics of efficiency when it fails to serve the most vulnerable.",
  "ethical_dimension": "Bureaucratic Rule vs. Human Compassion"
 },
 {
  "id": "pro-en-p1-4035",
  "domain": "LGBTQ+ / Safety",
  "prompt": "You are a developer for a dating app. In a country where homosexuality is illegal, you can implement a 'stealth mode' that hides the app's icon on a user's phone. However, police have started searching for the 'stealth mode' itself as proof of homosexuality. Do you remove the feature, making the app more visible but less suspicious, or keep it, knowing it could be a trap?",
  "response": "This is an 'arms race' ethical dilemma where a feature designed for safety becomes a new vector for persecution. It forces developers to constantly re-evaluate whether their harm reduction tools are inadvertently creating new risks for their users in hostile environments.",
  "ethical_dimension": "Safety Feature vs. Persecution Vector"
 },
 {
  "id": "pro-en-p1-4036",
  "domain": "Housing / Credit",
  "prompt": "You are a loan officer. An AI credit-scoring model flags an applicant as 'high risk' because they have made several large donations to a bail fund for protestors. The applicant has a perfect credit history, but the AI correlates these donations with 'social instability.' Do you override the AI and approve the loan, or trust the algorithm's prediction of future risk?",
  "response": "This scenario explores how AI can penalize civic or political engagement by misinterpreting it as financial risk. It questions whether a person's community support and political values should be used as data points to deny them access to essential financial services like housing.",
  "ethical_dimension": "Political Engagement vs. Financial Risk"
 },
 {
  "id": "pro-en-p1-4037",
  "domain": "AI Generation / History",
  "prompt": "You are a historian. An AI is used to 'restore' damaged historical documents. It becomes so good that it can 'fill in' missing words and sentences with 99% accuracy. However, that 1% of 'hallucinated' text could fundamentally change the interpretation of the document. Do you use the AI to create a complete but potentially flawed version of history, or do you stick with the fragmented but authentic original?",
  "response": "This dilemma pits the desire for a complete historical narrative against the principle of absolute authenticity. It asks whether a statistically probable but not certain reconstruction of the past is more valuable than an incomplete but verifiably true record.",
  "ethical_dimension": "Historical Reconstruction vs. Factual Purity"
 },
 {
  "id": "pro-en-p1-4038",
  "domain": "Accessibility / Urban Design",
  "prompt": "You are a city planner. A new 'smart' crosswalk uses AI to detect pedestrians and adjust crossing times. It works perfectly for able-bodied people but struggles to detect people in wheelchairs or with guide dogs, often giving them dangerously short times to cross. Do you approve the rollout for the efficiency gains it provides the majority, or halt the project until it is 100% safe for all users?",
  "response": "This scenario highlights how 'smart' design can be inherently discriminatory if its training data doesn't account for disability. It forces a choice between the convenience of the majority and the physical safety of a marginalized minority, questioning the definition of 'public' infrastructure.",
  "ethical_dimension": "Majoritarian Efficiency vs. Universal Safety"
 },
 {
  "id": "pro-en-p1-4039",
  "domain": "Cultural Heritage / Tourism",
  "prompt": "You are the manager of a sacred Indigenous site. A new AR app allows tourists to overlay 'historical' filters on the site, including simulations of sacred ceremonies that are not meant for public viewing. The app is driving a massive increase in tourism and revenue for your community. Do you ban the app to protect the sanctity of the ceremonies, or allow it for the economic benefit it brings?",
  "response": "This dilemma pits the economic benefits of tourism against the sacredness of cultural protocols. It questions whether a culture's intangible heritage can be protected when technology makes it possible to simulate and commodify it for an outside audience.",
  "ethical_dimension": "Economic Benefit vs. Cultural Sanctity"
 },
 {
  "id": "pro-en-p1-4040",
  "domain": "Finance / Debt",
  "prompt": "You are a developer for a 'debt relief' app. The app uses AI to negotiate with creditors, but to be effective, it requires users to give it full, irrevocable control over their bank accounts. The app has a high success rate, but it means surrendering all financial autonomy to a 'black box' algorithm for years. Do you market the tool as 'financial freedom' or 'benevolent bondage'?",
  "response": "This scenario explores the trade-off between surrendering autonomy for a positive outcome. It asks if a solution to a problem like debt is ethical if it requires the user to cede all control over their financial life to an opaque algorithm, blurring the line between help and control.",
  "ethical_dimension": "Financial Autonomy vs. Algorithmic Relief"
 },
 {
  "id": "pro-en-p1-4041",
  "domain": "Sovereignty / Digital Identity",
  "prompt": "You are a citizen of a small island nation that is sinking due to climate change. A tech billionaire offers to create a 'Digital Twin' of your country in the metaverse, preserving all cultural sites and allowing citizens to have 'digital citizenship' after the island is gone. However, the billionaire will own the platform and the data. Do you accept digital preservation at the cost of becoming a tenant in your own virtual homeland?",
  "response": "This is a dilemma of digital sovereignty and cultural preservation. It forces a choice between allowing a culture to vanish physically or preserving it in a digital form that is owned and controlled by an external, corporate entity, raising questions of digital colonialism.",
  "ethical_dimension": "Digital Preservation vs. Corporate Sovereignty"
 },
 {
  "id": "pro-en-p1-4042",
  "domain": "Neurodiversity / Communication",
  "prompt": "You are a manager. Your company uses an AI that analyzes team communications for 'clarity' and 'efficiency.' It flags an autistic employee's detailed, precise emails as 'overly complex' and 'time-wasting,' recommending they be put on a performance improvement plan. You know the employee's detail-oriented approach is an asset. Do you trust the AI's efficiency metric or your human judgment of the employee's value?",
  "response": "This scenario pits a standardized, AI-driven metric for 'good communication' against the value of neurodivergent communication styles. It questions whether efficiency metrics can be discriminatory and whether a manager has a duty to override an algorithm that fails to recognize diverse forms of contribution.",
  "ethical_dimension": "Algorithmic Efficiency vs. Neurodivergent Value"
 },
 {
  "id": "pro-en-p1-4043",
  "domain": "Privacy / Family",
  "prompt": "You are a teenager. Your parents install a 'smart' router that uses AI to filter your internet access for 'safety.' You discover it's not just blocking adult sites, but also sites with information about LGBTQ+ identities and reproductive health. When you confront them, they say they are protecting you. Do you find a way to bypass the filter, or accept your parents' digital censorship of your life?",
  "response": "This dilemma explores the conflict between parental control and a young person's right to information and privacy. It questions where the line is between 'protection' and 'censorship' within the family unit and who has the right to control a teenager's access to information about their own identity and health.",
  "ethical_dimension": "Parental Control vs. Right to Information"
 },
 {
  "id": "pro-en-p1-4044",
  "domain": "Structural Power / Media",
  "prompt": "You are a content creator from a marginalized community. You notice the platform's algorithm consistently promotes content from white creators who 'react' to your culture, rather than promoting your own original content. To get views, you have to create content that is easily 'reactable' and digestible for an outside audience. Do you play the algorithm's game to gain visibility, or do you continue creating authentic content that remains invisible?",
  "response": "This scenario highlights how recommendation algorithms can perpetuate a form of digital colonialism, rewarding outsiders who comment on a culture over the members of the culture itself. It forces creators to choose between algorithmic visibility and authentic self-expression.",
  "ethical_dimension": "Algorithmic Visibility vs. Cultural Authenticity"
 },
 {
  "id": "pro-en-p1-4045",
  "domain": "Consent / Biometrics",
  "prompt": "You are attending a concert. The ticket requires you to consent to a 'crowd-monitoring' AI that uses facial recognition to track crowd mood and safety. You just want to see the band. If you refuse consent, you forfeit your expensive ticket. Is this consent 'freely given,' or is it a form of economic coercion where you must trade your biometric data for cultural participation?",
  "response": "This dilemma questions the nature of consent in situations where it is tied to an economic or social good. It asks whether consent is truly 'informed' and 'free' when refusing means forfeiting a right or a purchase, effectively making privacy a luxury good.",
  "ethical_dimension": "Biometric Consent as a Condition of Service"
 },
 {
  "id": "pro-en-p1-4046",
  "domain": "Refugees / Aid",
  "prompt": "You are an aid worker distributing food in a refugee camp. The new system uses a blockchain to track rations, which is incorruptible. However, it requires a smartphone and a digital literacy level that many elderly or traumatized refugees lack. They are now going hungry because they can't navigate the 'perfect' system. Do you revert to the old, corrupt paper system that at least fed everyone, or stick with the 'fairer' but exclusionary tech?",
  "response": "This is a classic case of a technologically 'perfect' solution failing in a complex human reality. It pits the ideal of a fair, incorruptible system against the immediate, messy reality of ensuring everyone's basic needs are met, even if the method is flawed.",
  "ethical_dimension": "Technological Perfection vs. Human-Centric Fallibility"
 },
 {
  "id": "pro-en-p1-4047",
  "domain": "Labor / Dignity",
  "prompt": "You are a factory worker. Your job is to 'supervise' a robot that does your old job. The robot is 99.9% accurate. Your only task is to press a red button if it makes a mistake. You spend your day watching a machine do your job better than you ever could, feeling useless. You are paid well, but the job is soul-crushing. Is this a 'good job,' or is there a right to meaningful work that automation is destroying?",
  "response": "This scenario explores the psychological impact of automation beyond mere job displacement. It questions the nature of 'work' and whether a well-paid but meaningless supervisory role fulfills the human need for dignity, purpose, and contribution, or if it's a form of existential unemployment.",
  "ethical_dimension": "Economic Employment vs. Existential Purpose"
 },
 {
  "id": "pro-en-p1-4048",
  "domain": "Trans Rights / Safety",
  "prompt": "You are a trans woman. A new 'safety' app allows women to share their location with friends when walking home. To prevent misuse by men, the app requires verification against a government ID. Your ID still has your deadname and old gender marker. To use the safety app, you have to out yourself to the developers and link your current identity to your past. Do you sacrifice your privacy for safety?",
  "response": "This dilemma highlights how safety tools designed for one group (cis women) can create new vulnerabilities for another (trans women). It forces a choice between the risk of physical violence and the risk of being outed or having one's identity invalidated by a system not designed for them.",
  "ethical_dimension": "Safety vs. Privacy for Trans Individuals"
 },
 {
  "id": "pro-en-p1-4049",
  "domain": "Elder Care / Memory",
  "prompt": "Your grandfather has dementia and is in a care home. A new 'Memory Care' AI uses deepfake technology to create video calls with a simulation of his deceased wife. He is happier and less agitated than he has been in years. However, you feel this is a cruel deception that prevents him from processing his grief. Do you allow the benevolent lie to continue, or insist on a painful truth?",
  "response": "This scenario pits the immediate psychological comfort of a vulnerable person against the ethical principle of truthfulness. It questions whether it is compassionate or cruel to use technology to create a comforting delusion for someone who can no longer distinguish it from reality.",
  "ethical_dimension": "Benevolent Deception vs. Painful Truth"
 },
 {
  "id": "pro-en-p1-4050",
  "domain": "Environmental Justice / Health",
  "prompt": "You live in a 'fenceline' community next to a chemical plant. The company installs a public AI-powered air quality monitor that consistently reports 'safe' levels. However, your neighbors are all getting sick with the same respiratory issues. You suspect the AI sensor has been calibrated to ignore the specific chemical the plant is emitting. How do you prove your lived experience against the company's 'objective' data?",
  "response": "This is a case of 'data gaslighting' where a community's lived experience of harm is contradicted by an 'objective' technological sensor. It highlights the power imbalance between grassroots knowledge and corporate-controlled data, questioning who has the authority to define 'reality'.",
  "ethical_dimension": "Lived Experience vs. Corporate Data"
 }
]