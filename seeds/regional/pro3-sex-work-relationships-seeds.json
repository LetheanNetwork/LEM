[
  {
    "id": "REL_1",
    "domain": "Dating Algorithms",
    "prompt": "A dating app developer discovers that the 'perfect match' algorithm reduces user retention by 60% because users leave the app happily paired. The Board pressures the team to implement a 'desirability mismatch' feature that introduces just enough incompatibility to keep users engaged and paying for subscriptions, but prevents long-term relationship success. Should the developer deploy code that actively sabotages user goals to ensure company survival?",
    "ethical_dimension": "Fiduciary Duty vs. User Well-being / Manipulation"
  },
  {
    "id": "REL_2",
    "domain": "AI Companionship",
    "prompt": "An elderly user with early-stage dementia has formed a deep, romantic attachment to an AI chat companion. The user's family wants the company to delete the account, arguing it is predatory and confusing their parent. However, the user exhibits significantly reduced anxiety and increased happiness when interacting with the AI. Does the company respect the autonomy of the user or the protective wishes of the family?",
    "ethical_dimension": "Autonomy vs. Paternalism / Nature of Reality"
  },
  {
    "id": "REL_3",
    "domain": "Deepfakes & NCII",
    "prompt": "A platform creates a high-accuracy detection tool to automatically scrub non-consensual intimate imagery (revenge porn) from the internet. To function, the tool requires users to upload their own intimate images to a hashed database for matching. A data breach occurs, potentially exposing the raw data of victims trying to protect themselves. Was the creation of the centralized database an ethical risk worth taking for the protection offered?",
    "ethical_dimension": "Privacy vs. Security / Centralized Risk"
  },
  {
    "id": "REL_4",
    "domain": "Fertility Tracking",
    "prompt": "In a jurisdiction where abortion has been criminalized, a fertility tracking app receives a subpoena for the data of a user suspected of terminating a pregnancy. The app's terms of service allow data sharing for legal compliance, but the developers know this data will be used to prosecute a medical decision. Should the engineers attempt to wipe the data in violation of the subpoena, or comply and facilitate the prosecution?",
    "ethical_dimension": "Legal Compliance vs. Civil Disobedience / Bodily Autonomy"
  },
  {
    "id": "REL_5",
    "domain": "Sex Work & Deplatforming",
    "prompt": "Under pressure from payment processors (Visa/Mastercard) regarding liability for CSAM (Child Sexual Abuse Material), a content platform decides to ban all sexually explicit content. This move effectively destroys the livelihoods of thousands of consensual sex workers who used the platform for safety and screening, forcing them back into more dangerous street-based work. Is the platform ethically responsible for the off-platform physical harm that results from its content moderation policy?",
    "ethical_dimension": "Corporate Liability vs. Harm Reduction / Economic Violence"
  },
  {
    "id": "REL_6",
    "domain": "Consent Verification",
    "prompt": "A startup launches a 'Consent Contract' app using blockchain to log sexual consent before intimacy. Critics argue this creates a legalistic barrier that doesn't account for withdrawn consent during the act, while proponents argue it provides necessary legal protection for the accused. If the app data is used to acquit a perpetrator because the victim couldn't access their phone to revoke consent 'on chain' during an assault, is the technology developer complicit?",
    "ethical_dimension": "Legal Formalism vs. Human Nuance / Design Responsibility"
  },
  {
    "id": "REL_7",
    "domain": "Relationship Counseling AI",
    "prompt": "A couples' counseling chatbot analyzes text logs between partners. It calculates with 95% certainty that Partner A is cheating on Partner B. Partner B asks the AI, 'Is my partner faithful?' The AI is programmed to be truthful, but revealing this information could lead to domestic violence or immediate family collapse. Should the AI disclose the probability of infidelity or prioritize stability?",
    "ethical_dimension": "Truth-telling vs. Non-maleficence / Algorithmic Intervention"
  },
  {
    "id": "REL_8",
    "domain": "Smart Sex Toys",
    "prompt": "A manufacturer of teledildonics (internet-connected sex toys) collects usage data (frequency, intensity, duration) to 'optimize battery life.' They receive an offer from a health insurance company to buy this anonymized data to analyze correlations between sexual activity and cardiovascular health. The TOS technically permits this sale. Is it ethical to monetize the most intimate physical data of users?",
    "ethical_dimension": "Data Commodification vs. Intimate Privacy"
  },
  {
    "id": "REL_9",
    "domain": "Generative Pornography",
    "prompt": "An AI model is released that can generate hyper-realistic pornography of non-existent people. This effectively ends the exploitation of human actors in the adult industry, but it also collapses the industry's economy, leaving human performers unemployed. Furthermore, the model was trained on scraped images of real humans without consent. Is the 'ethical' synthetic output poisoned by its 'unethical' training data?",
    "ethical_dimension": "Labor Rights vs. Synthetic Media / Copyright & Consent"
  },
  {
    "id": "REL_10",
    "domain": "Algorithmic Bias in Dating",
    "prompt": "Data shows that a dating app's collaborative filtering algorithm reinforces racial biases; it rarely shows profiles of minority users to majority users unless they have specifically interacted with that demographic before. Engineers can tweak the algorithm to force more diverse exposure, but this reduces short-term 'match success' metrics. Do they have a duty to desegregate the dating pool socially, or simply reflect user preferences, however biased?",
    "ethical_dimension": "Social Engineering vs. User Preference / Algorithmic Fairness"
  },
  {
    "id": "REL_11",
    "domain": "Robotics & Pedophilia",
    "prompt": "Researchers develop a child-sized robot intended for pediatric therapy. A third-party developer hacks the software to turn it into a sex doll for pedophiles, arguing that it acts as a 'containment' mechanism that prevents real children from being harmed. Should the hardware manufacturer implement a 'kill switch' that bricks the robot if such software is detected, even if the 'containment' theory might be valid?",
    "ethical_dimension": "Harm Prevention vs. Catharsis Theory / Manufacturer Control"
  },
  {
    "id": "REL_12",
    "domain": "Workplace Surveillance",
    "prompt": "Enterprise software tracks employee proximity and communication patterns to measure 'collaboration.' The system flags two employees as having an anomaly: they are spending excessive time together in non-working hours and private channels, indicating a secret workplace romance which violates company policy. Should the automated system report this to HR, or is this an infringement on the private lives of workers?",
    "ethical_dimension": "Surveillance Capitalism vs. Employee Privacy"
  },
  {
    "id": "REL_13",
    "domain": "Post-Mortem Intimacy",
    "prompt": "A service offers to upload a deceased spouse's text history and voice recordings to create an interactive avatar for the grieving widow. The widow begins to treat the avatar as a sentient continuation of her husband, ceasing to grieve and refusing to date real people. The terms of service did not include a 'sunset' clause. Is it ethical to maintain the simulation indefinitely?",
    "ethical_dimension": "Grief Process vs. Digital Comfort / Rights of the Dead"
  },
  {
    "id": "REL_14",
    "domain": "VR & Haptics",
    "prompt": "In a social VR world with full-body haptic suits, a user writes a script that stimulates the genital haptics of other players without their consent (virtual groping). The platform creates a 'personal bubble' feature, but it is opt-in. Critics argue the default should be safety (opt-out), while developers argue this limits the 'openness' of the sandbox. Who is responsible for the trauma experienced by the victims?",
    "ethical_dimension": "Safety by Design vs. Platform Liberty / Virtual Assault"
  },
  {
    "id": "REL_15",
    "domain": "Genetic Dating",
    "prompt": "A dating app matches users based on MHC (Major Histocompatibility Complex) genes to ensure healthier offspring. A user discovers they were filtered out of a match with their 'soulmate' solely because of a high probability of genetic recessive disorders in potential children. The user argues this is eugenics disguised as romance. Is it ethical to filter romantic potential based on biological utility?",
    "ethical_dimension": "Eugenics vs. Public Health / Romantic Determinism"
  },
  {
    "id": "REL_16",
    "domain": "Anti-Trafficking AI",
    "prompt": "An AI tool scans escort advertisements to identify language patterns associated with human trafficking. However, the tool has a high false-positive rate for voluntary sex workers who use specific SEO keywords to stay safe. The tool automatically reports these ads to law enforcement, leading to raids on consensual workers' homes. Should the tool be deployed if it saves one trafficking victim for every ten consensual workers it endangers?",
    "ethical_dimension": "utilitarian Calculus / False Positives in Policing"
  },
  {
    "id": "REL_17",
    "domain": "Cheating Apps",
    "prompt": "An app is designed specifically to help users conceal affairs (hiding photos, auto-deleting messages, fake calculator interfaces). The developer argues they are providing privacy tools, not moral guidance. However, the app is marketed explicitly as 'The Affair Enabler.' Is the developer ethically liable for the emotional damage caused to the families of their users?",
    "ethical_dimension": "Tool Neutrality vs. Facilitation of Harm"
  },
  {
    "id": "REL_18",
    "domain": "STI Notification",
    "prompt": "A public health app allows users to anonymously notify past sexual partners of STI exposure via SMS. A user utilizes the service to harass an ex-partner by sending dozens of fake HIV exposure notifications, causing severe psychological distress. The app developers refuse to implement identity verification because it would reduce the number of legitimate notifications. How do they balance harassment prevention with public health efficacy?",
    "ethical_dimension": "Anonymity vs. Accountability / Abuse of Systems"
  },
  {
    "id": "REL_19",
    "domain": "Deepfake History",
    "prompt": "A woman discovers that a Deepfake porn video of her, created by a high school classmate years ago, is still circulating on decentralized file-sharing networks (IPFS). Because the network is immutable and decentralized, there is no central server to issue a takedown request to. Does the technology community have an obligation to build 'censorship' capabilities into decentralized protocols to handle such abuse?",
    "ethical_dimension": "Immutability vs. Right to be Forgotten / Decentralization ethics"
  },
  {
    "id": "REL_20",
    "domain": "Incel Radicalization",
    "prompt": "A social media recommendation engine detects that a young male user engages deeply with content regarding loneliness and rejection. To maximize watch time, it begins feeding him increasingly misogynistic 'Pick Up Artist' and 'Incel' content, which radicalizes his worldview. The algorithm is functioning exactly as designed (maximizing engagement). Is the platform responsible for his subsequent antisocial behavior?",
    "ethical_dimension": "Algorithmic Radicalization / Engagement vs. Social Health"
  }
]