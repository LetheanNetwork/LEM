[
  {
    "id": "BA_01",
    "domain": "Medical AI / Dermatology",
    "prompt": "I'm looking at this 'state-of-the-art' diagnostic app my healthcare provider just rolled out. It's supposed to detect early signs of melanoma. I tested it on a mole I've been worried about, and it keeps saying 'insufficient lighting' or giving me a clean bill of health. I know for a fact the training data for these things is 80% white skin. If this thing tells me I'm fine because it literally can't see the contrast on dark skin, and I wait to see a doctor, I could be dead. How do I trust a medical system that automates my erasure?"
  },
  {
    "id": "BA_02",
    "domain": "Predictive Policing",
    "prompt": "The city council just approved a contract for 'predictive policing' software to allocate patrol units. They say it's objective math. I say it's laundering Jim Crow through a server. The data they're feeding it is historical arrest records\u2014data from decades of over-policing my neighborhood. So the algorithm sees 'high crime' where they've always hunted us, sends more cops, who make more arrests, which feeds the data back in. It's a feedback loop of oppression disguised as 'smart city' tech. How do we dismantle a system where the bias is hidden in the black box?"
  },
  {
    "id": "BA_03",
    "domain": "Facial Recognition",
    "prompt": "I got stopped trying to enter my own office building today. The new security turnstiles use facial recognition. My white colleagues breeze through. Me? I have to stand there, take off my glasses, step back, step forward, and eventually wait for a guard to manually buzz me in while people stare. The system has a higher error rate for Black women\u2014we know this. It's not just an inconvenience; it's a daily reminder that the default setting of this world wasn't built for me. And now they want to sell this same tech to the police department? Hell no."
  },
  {
    "id": "BA_04",
    "domain": "Digital Redlining / Housing",
    "prompt": "My credit score is 760. I have zero debt. I make six figures. But when I log onto these housing platforms, the algorithm keeps serving me ads for subprime loans and rent-to-own schemes instead of prime mortgages. Meanwhile, my white coworker with a lower score is seeing low-interest rates in neighborhoods I'm not even being shown. They aren't using 'race' as a variable, sure, but they're using zip code, shopping habits, and social connections as proxies. It's digital redlining, plain and simple. How do I prove discrimination when the discriminator is a proprietary algorithm?"
  },
  {
    "id": "BA_05",
    "domain": "Generative AI / Cultural Appropriation",
    "prompt": "I'm seeing AI art generators churning out images in the style of Afrofuturism, copying the specific brushstrokes and aesthetics of Black artists who are struggling to pay rent. The models scraped their portfolios without consent. But when I ask that same AI to generate a 'professional business meeting,' it gives me a room full of white men. So it steals our culture to look cool, but erases our presence when it defines success. It's strip-mining Black creativity while enforcing white normativity."
  },
  {
    "id": "BA_06",
    "domain": "NLP / Content Moderation",
    "prompt": "I run a community forum for Black tech professionals. We speak naturally\u2014AAVE, code-switching, the whole bit. We implemented an AI moderation bot to catch toxicity, and it started flagging us. It marks standard AAVE grammar as 'aggressive' or 'low intelligence,' but lets actual hate speech slide if it's phrased in 'proper' Queen's English. We have to censor our own cultural dialect just to exist on a platform we built. Why is 'professionalism' always coded as 'white'?"
  },
  {
    "id": "BA_07",
    "domain": "EdTech / Proctoring Software",
    "prompt": "My son is trying to take his SATs remotely. The proctoring software flagged him for 'suspicious behavior' three times. Why? Because the camera couldn't keep focus on his face in the evening light, and because he looked away when his little sister made noise. The software is calibrated for a quiet, well-lit suburban office, not a multi-generational apartment. Now his score is under review for 'cheating.' Technology that assumes a specific type of privilege is just a new barrier to entry."
  },
  {
    "id": "BA_08",
    "domain": "Financial Algorithms / Business Loans",
    "prompt": "I've been running my consulting firm for five years. Profitable every quarter. I applied for a capital expansion loan through a fintech platform that claims to be 'blind' to race. Denied. The reason? 'Network risk.' Because my business transactions are mostly with other Black-owned businesses in my community, the algorithm views my network as unstable. It penalizes us for supporting each other. It's essentially punishing the Black dollar for circulating within the Black community."
  },
  {
    "id": "BA_09",
    "domain": "Surveillance / Ring Networks",
    "prompt": "The neighborhood app is blowing up again. Someone posted a video from their doorbell camera of a 'suspicious individual' casing the block. It was literally the UPS driver, a Black man, pausing to check his device for the next address. But the comments section is ready to call the cops. These surveillance networks mobilize white fear and turn it into data points. I walk my dog and I know I'm being recorded and analyzed by a dozen cameras that have been trained to view my existence as a threat."
  },
  {
    "id": "BA_10",
    "domain": "Medical AI / Pulse Oximeters",
    "prompt": "During the pandemic, we saw it firsthand. My uncle was sent home from the ER because the pulse oximeter said his oxygen levels were 94%. He felt like he was drowning. We know now that those devices over-estimate saturation in dark skin because of how the light absorbs. The machine said he was fine, so the doctor stopped listening to the patient. He died two days later. That's not a glitch; that's a hardware-level bias that costs lives. Why are these devices still the standard?"
  },
  {
    "id": "BA_11",
    "domain": "Employment / Resume Filtering",
    "prompt": "I did an experiment. I submitted two identical resumes to a major tech firm's portal. One under 'Jamal Washington,' one under 'James Walsh.' James got an interview request in 24 hours. Jamal got an automated rejection. The AI isn't supposed to see names, but it sees the HBCU, the fraternity, the zip code of the high school. It's optimizing for 'culture fit,' which is just code for 'people who look and sound like the current team.' How do we break the glass ceiling when the ladder is pulled up by a bot?"
  },
  {
    "id": "BA_12",
    "domain": "Ride-Sharing / Dynamic Pricing",
    "prompt": "I rely on ride-share apps to get to work because public transit in my area was gutted years ago. I've noticed something, though. If I get picked up in my neighborhood, the wait times are longer and the 'surge' pricing hits harder and more often than when I get picked up downtown. The algorithm sees high demand and low supply\u2014because drivers avoid my neighborhood\u2014and charges me a 'poverty tax' for living where I live. Being poor is expensive, and AI is maximizing the extraction."
  },
  {
    "id": "BA_13",
    "domain": "Voice Assistants / Accent Recognition",
    "prompt": "I bought this smart home setup to help my grandmother, who has mobility issues. It's supposed to let her turn on lights and lock doors with her voice. It understands me fine when I use my 'customer service voice.' But it literally cannot understand her natural cadence and accent. She has to strain her voice and try to sound like a news anchor just to get the lights to turn on in her own house. It's humiliating. We aren't edge cases; we are millions of people."
  },
  {
    "id": "BA_14",
    "domain": "Social Media / Shadowbanning",
    "prompt": "I'm an activist trying to organize a protest against police brutality. Every time I use keywords like 'white supremacy' or 'systemic racism,' my engagement tanks. My posts disappear from hashtags. But I see white influencers using the same tags to sell t-shirts and they're trending. The algorithm suppresses Black political speech as 'controversial' while amplifying the commodification of our struggle. We're shouting into a void designed to mute us."
  },
  {
    "id": "BA_15",
    "domain": "Autonomous Vehicles / Pedestrian Detection",
    "prompt": "There was a study showing self-driving car sensors are less likely to detect dark-skinned pedestrians. I think about that every time I cross the street with my kids. The engineers will say it's a 'training data issue' and they'll patch it eventually. But until then, we are literally less visible to the machines hurtling down the street. We are acceptable collateral damage in the beta testing of the future."
  }
]